<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Ruben Fiszel" />
  <title>Master thesis Pt I: Accelerated optimal sensor fusion algorithm for POSE estimation of drones: Asynchronous Rao-Blackwellized Particle filter</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="markdown3.css" type="text/css" />
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title">Master thesis Pt I: Accelerated optimal sensor fusion algorithm for POSE estimation of drones: Asynchronous Rao-Blackwellized Particle filter</h1>
<h2 class="author">Ruben Fiszel</h2>
<h3 class="date">June 2017</h3>
</div>
<p>This post is the part I out of III of my master thesis at the <a href="http://dawn.cs.stanford.edu/">DAWN lab</a>, Stanford. The central themes of this thesis are sensor fusion and spatial, an hardware accelerator language (Verilog is also one). This part is about an application of hardware acceleration, sensor fusion for drones. Part II will be about <a href="https://github.com/rubenfiszel/scala-flow/">scala-flow</a>, a library made during my thesis as a development tool for Spatial inspired by Simulink. This library helped me greatly in develop the filter but is intended to be very general purpose. Part III will be the development of an interpreter for spatial, and the spatial implementation of the filters presented in Part I. If you are only interested in the filter, you can skip the introduction.</p>
<h1 id="introduction">Introduction</h1>
<h2 id="moores-law-end">Moore’s law end</h2>
<p>The Moore’s law<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> has ruled computation for the last 4 decades. With each generation of processor, the promise of an exponentially faster execution. Transistors are reaching the scale of 10nm, only a 100 time bigger than an atom. Unfortunately, the quantum rules of physics which govern the infinitesimally, start manifest themselves. In particular, quantum tunneling move electrons from classicly unsurmountable barrier, making computations approximate, containing a non negligible fraction of errors.</p>
<div class="figure">
<img src="moorelaw.png" alt="A graph that shows the number of transistors through the years. We can see the glimpse of the recent decline" />
<p class="caption">A graph that shows the number of transistors through the years. We can see the glimpse of the recent decline</p>
</div>
<h2 id="the-rise-of-hardware">The rise of Hardware</h2>
<p>Hardware and Software designate here respectively programs that are executed as code for a general purpose processing unit and programs that are encoded in the circuits. The dichotomy is not very well defined and we can think of it as a spectrum. General-purpose computing on graphics processing units (GPGPU) is in-between. Very efficient when appropriate and used well. They have benefited from high-investment and many generation of iterations and hence, for some tasks, can rivalize or even surpass Hardware.</p>
<div class="figure">
<img src="hwsf.jpg" alt="Hardware vs Software" />
<p class="caption">Hardware vs Software</p>
</div>
<p>Hardware has always been there but application-specific integrated circuit (ASIC) has prohibitive costs upfront (in the range of $100M for a tapeout). Reprogrammable hardware like field-programmable gate array (FPGA) have only been used marginally and for some specific industry like high-frequency trading. But now Hardware might be the only solution (until a computing revolution happen, like quantum computing, but this is not realist for the near future) to increase performance. But hardware do not enjoy the same quality of tool, language and integrated development environment (IDE) as software. This is the motivation behind Spatial.</p>
<h2 id="hardware-as-companion-accelerators">Hardware as companion accelerators</h2>
<p>In most case, hardware would be inappropriate: running an OS as hardware would be irrealist. However, as a companion to a central-processing unit (CPU also called “the host”), you are able to get the best of both world. The flexibility of software on a CPU with the speed of hardware. In this setup, hardware is considered an “accelerator” (Hence, the term “accelerating hardware”). It accelerates the most demanding subroutines of the CPU. This companionship is already present in modern computer desktops under the form of GPUs for <em>shader</em> operations and sound card for complex sound transformation/output.</p>
<h2 id="the-right-metric-perfwatt">The right metric: Perf/Watt</h2>
<p>The right metric for accelerator is performance by energy, as measured in FLOPS per Watt. This is a fair metric for the comparison of different hardware because it shows the intrisic value of the architecture. If the metric was solely performance, then it would suffice to combine multiple of the same architecture. Perf per dollar is not a good metric either because you should also account for the cost of energy at runtime. Hence, Perf/Watt seems like a fair metric to compare architectures.</p>
<h2 id="spatial">Spatial</h2>
<p>At the dawn lab, under the lead of <a href="http://arsenalfc.stanford.edu/kunle">Prof. Kunle</a> and his grad students, is developped a scala DSL <a href="https://github.com/stanford-ppl/spatial-lang">spatial</a> and its compiler to program Hardware in a higher-level, more user-friendly, more productive language than Verilog. In particular, the control flows are automatically generated when possible. This should enable software engineers to unlock the potential of Hardware. A custom CGRA, Plasticine, has been developped in parralel to Spatial. It leverages some recurrent patterns, in particular parralel patterns and aims to be the most efficient reprogrammable architecture for Spatial.</p>
<p>There is a large upfront cost but once at a big enough scale, Plasticine could be deployed as an accelerator for most demanding server applications and embedded systems with heavy computing requirements.</p>
<h2 id="embedded-systems-and-drones">Embedded systems and drones</h2>
<p>Embedded systems are limited by the amount of power at disposal from the battery and might also have size constraints. At the same time, especially for autonomous vehicles, there is a great need for computing power.</p>
<p>Thus, developping drone applications with spatial demonstrates the advantages of the platform. As a matter of fact, the filter that has been developped was only made possible because it was run on an accelerating hardware. It would be irrealist to attempt to run it on more conventional micro-transistors. This is why the family in which belong the filter developped here, particles filters, being very computationally expensive, are very seldom used for drones.</p>
<h1 id="part-i-accelerated-optimal-sensor-fusion-algorithm-for-pose-estimation-of-drones-asynchronous-rao-blackwellized-particle-filter">Part I: Accelerated optimal sensor fusion algorithm for POSE estimation of drones: Asynchronous Rao-Blackwellized Particle filter&quot;</h1>
<p>POSE is the combination of the position and orientation of an object. POSE estimation is important for drones. It is a subroutine of SLAM (Simultaneous localization and mapping) and it is a central part of motion planning and motion control. More accurate and more reliable POSE estimation results in more agile, more reactive and safer drones. Drones are an intellectually stimulating subject but in the near-future they might also see their usage increase exponentially. In this context, developping and implementing new filter for POSE estimation is both important for the field of robotics but also to demonstrate the importance of hardware acceleration. Indeed, the best and last filter presented here is only made possible because it can be hardware accelerated with Spatial. However, the spatial implementation will be presented in Part III.</p>
<p>Before expanding on the Rao-Blackwellized particle filter, we will introduce here several other filters for POSE estimation for highly dynamic objects: Complementary filter, Kalman Filter, Extended Kalman Filter and finally Rao-Blackwellized Particle filter. The order is from the most conceptually simple, to the most complex. This order is justified because complex filters aim to alleviate some of the flaws of their simpler counterpart. It is important to understand what are those weakness and how we can alleviate them.</p>
<h2 id="drones-and-collision-avoidance">Drones and collision avoidance</h2>
<p>The original motivation for the development of accelerated POSE estimation is for the task of collision avoidance by quadcopters. In particular, a collision avoidance algorithm developped at the <a href="https://asl.stanford.edu/">ASL lab</a> and demonstrated here <a href="https://www.youtube.com/watch?v=kdlhfMiWVV0">(https://youtu.be/kdlhfMiWVV0)</a></p>
<div class="figure">
<img src="fencing.png" alt="Ross Allen fencing with his drone" />
<p class="caption">Ross Allen fencing with his drone</p>
</div>
<p>where the drone avoids the sword attack froms its creator. At first, it was thought of accelerating the whole algorithm but it was found that one of the most demanding subroutine was pose estimation. Moreover, it was wished to increase the processing rate of the filter such that it could match the input with the fastest sampling rate: its inertial measurement unit (IMU) containing an accelerometer, a gyroscope and a magnetometer.</p>
<p>The flamewheel f450 is the typical drone in this category. It is surprisingly fast and agile. Given the proper command, it can generate enough thrust to avoid in a very short lapse of time any incoming object.</p>
<div class="figure">
<img src="f450.jpg" alt="The Flamewheel f450" />
<p class="caption">The Flamewheel f450</p>
</div>
<h2 id="sensor-fusion">Sensor fusion</h2>
<p>Sensor fusion is combining of sensory data or data derived from disparate sources such that the resulting information has less uncertainty than would be possible when these sources were used individually. In the context of drones, it is very useful because it enables to combine many unprecise sensor measurement to form a more precise measurement like having precise positionning from 2 less precise GPS (dual GPS setting). It can also permit to combine sensors with different sampling rates: typically precise sensors with low sampling rate and less precise sensors with high sampling rate. Both cases are gonna be relevant here.</p>
<p>A fundamental explanation why this is possible comes from the central limit theorem: one sample from a distribution with a low variance is as good as n sample from a distribution with variance <span class="math inline">\(n\)</span> times higher.</p>
<p><span class="math display">\[\mathbb{V}(X_i)=\sigma^2 ~~~~~ \mathbb{E}(X_i) = \mu\]</span> <span class="math display">\[\bar{X} = \frac{1}{n}\sum X_i\]</span> <span class="math display">\[\mathbb{V}(\bar{X}) = \frac{\sigma^2}{n}  ~~~~~ \mathbb{E}(\bar{X}) = \mu\]</span></p>
<h2 id="notes-on-notation-and-conventions">Notes on notation and conventions</h2>
<p>The referential by default is the fixed world frame.</p>
<ul>
<li><span class="math inline">\(\mathbf{x}\)</span> designates a vector</li>
<li><span class="math inline">\(x_t\)</span> is the random variable of x at time t</li>
<li><span class="math inline">\(x_{t1:t2}\)</span> is the product of the random variable of x between t1 included and t2 included</li>
<li><span class="math inline">\(x^{(i)}\)</span> designates the random variable x of the arbitrary particle i</li>
<li><span class="math inline">\(\hat{x}\)</span> designates an estimated variable</li>
</ul>
<h2 id="pose">POSE</h2>
<p>POSE is the task of estimating the position and orientation of an object through time. It is a subroutine of Software Localization And Mapping (SLAM). We can formelize the problem as:</p>
<p>At each timestep, find the best expectation of a function of the hidden variable state (position and orientation), from their initial distribution and the history of observable random variables (such as sensor measurements).</p>
<ul>
<li>The state <span class="math inline">\(\mathbf{x}\)</span></li>
<li>The function <span class="math inline">\(g(\mathbf{x})\)</span> such that <span class="math inline">\(g(\mathbf{x}_t) = (\mathbf{p}_t, \mathbf{q}_t)\)</span> where <span class="math inline">\(\mathbf{p}\)</span> is the position and <span class="math inline">\(\mathbf{q}\)</span> is the attitude as a quaternion.</li>
<li>The observable variable <span class="math inline">\(\mathbf{y}\)</span> composed of the sensor measurements <span class="math inline">\(\mathbf{z}\)</span> and the control input <span class="math inline">\(\mathbf{u}\)</span></li>
</ul>
<p>The algorithm inputs are:</p>
<ul>
<li>control inputs <span class="math inline">\(\mathbf{u}_t\)</span> (the commands sent to the flight controller)</li>
<li>sensor measurements <span class="math inline">\(\mathbf{z}_t\)</span> coming from different sensors with different sampling rate</li>
<li>information about the sensors (sensor measurements biases and matrix of covariance)</li>
</ul>
<h2 id="quaternion">Quaternion</h2>
<p>Quaternions are extensions of complex numbers but with 3 imaginary parts. Unit quaternions can be used to represent orientation, also referred to as attitude. Quaternions algebra make rotation composition simple and quaternions avoid the issue of gimbal lock. In all filters presented, they will be used to represent the attitude.</p>
<p><span class="math display">\[\mathbf{q} = (q.r, q.i, q.j, q.k)^t = (q.r, \boldsymbol{\varrho})^T\]</span></p>
<p>Quaternion rotations composition is: <span class="math inline">\(q_2 q_1\)</span> which results in <span class="math inline">\(q_1\)</span> being rotated by the rotation represented by <span class="math inline">\(q_2\)</span>. From this, we can deduce that angular velocity integrated over time is simply <span class="math inline">\(q^t\)</span> if <span class="math inline">\(q\)</span> is the local quaternion rotation by unit of time.</p>
<p>Rotation of a vector by a quaternion is done by: <span class="math inline">\(q v q^*\)</span> where <span class="math inline">\(q\)</span> is the quaternion representing the rotation, <span class="math inline">\(q^*\)</span> its conjugate and <span class="math inline">\(v\)</span> the vector to be rotated.</p>
<p>The distance of between two quaternions, useful as an error metric is defined by the squared Frobenius norms of attitude matrix differences <span class="citation">(Markley et al. <a href="#ref-markley_averaging_2007">2007</a>)</span>.</p>
<p><span class="math display">\[\| A(\mathbf{q}_1) - A(\mathbf{q}_2) \|^2_F = 6 - 2 Tr [ A(\mathbf{q}_1)A^t(\mathbf{q}_2) ]\]</span></p>
<p>where</p>
<p><span class="math display">\[A(\mathbf{q}) = (q.r^2 - \| \boldsymbol{\varrho} \|^2) I_{3 \times 3} + 2\boldsymbol{\varrho} \boldsymbol{\varrho}^T - 2q.r[\boldsymbol{\varrho} \times]\]</span></p>
<p><span class="math display">\[[\boldsymbol{\varrho} \times] = \left( \begin{array}{ccc}
0 &amp; -q.k &amp; q.j \\
q.k &amp; 0 &amp; -q.i \\
-q.j &amp; q.i &amp; 0 \\
\end{array} \right)\]</span></p>
<h2 id="helper-functions-and-matrices">Helper functions and matrices</h2>
<p>We introduce some helper matrices.</p>
<ul>
<li><span class="math inline">\(\mathbf{R}_{b2f}\{\mathbf{q}\}\)</span> is the body to fixed vector rotation matrix. It transforms vector in the body frame to the fixed world frame. It takes as parameter the attitude <span class="math inline">\(\mathbf{q}\)</span>.</li>
<li><span class="math inline">\(\mathbf{R}_{f2b}\{\mathbf{q}\}\)</span> is its inverse matrix (from fixed to body).</li>
<li><span class="math inline">\(\mathbf{T}_{2a} = (0, 0, 1/m)^T\)</span> is the scaling from thrust to acceleration (by dividing by the weight of the drone: <span class="math inline">\(\mathbf{F} = m\mathbf{a} \Rightarrow \mathbf{a} = \mathbf{F}/m)\)</span> and then multiplying by a unit vector <span class="math inline">\((0, 0, 1)\)</span></li>
<li><span class="math display">\[R2Q(\boldsymbol{\theta}) = (\cos(\| \boldsymbol{\theta} \| / 2), \sin(\| \boldsymbol{\theta} \| / 2) \frac{\boldsymbol{\theta}}{\| \boldsymbol{\theta} \|} )\]</span> is a function that convert from a local angle rotation to a local quaternion rotation. The definition of this function come from converting <span class="math inline">\(\boldsymbol{\theta}\)</span> to a body-axis angle, and then to a quaternion.</li>
<li><span class="math display">\[Q2R(\mathbf{q}) = (q.i*s, q.j*s, q.k*s) \]</span> is its inverse function where <span class="math inline">\(n = \arccos(q.w)*2\)</span> and <span class="math inline">\(s = n/\sin(n/2)\)</span></li>
<li><span class="math inline">\(\Delta t\)</span> is the lapse of time between t and the next tick (t+1)</li>
</ul>
<h2 id="model">Model</h2>
<p>The drone is assumed to have rigid-body physics. It is submitted to the gravity and its own inertia. A rigid body is a solid body in which deformation is zero or so small it can be neglected. The distance between any two given points on a rigid body remains constant in time regardless of external forces exerted on it. This enable to summarise the forces from the rotor as a thrust oriented in the direction normal to the plane formed by the 4 rotosrs, and an angular velocity.</p>
<p>Those variables are sufficient to describe the evolution of our drone with rigid-body physics:</p>
<ul>
<li><span class="math inline">\(\mathbf{a}\)</span> the total acceleration in the fixed world frame</li>
<li><span class="math inline">\(\mathbf{v}\)</span> the velocity in the fixed world frame</li>
<li><span class="math inline">\(\mathbf{p}\)</span> the position in the fixed world frame</li>
<li><span class="math inline">\(\boldsymbol{\omega}\)</span> the angular velocity</li>
<li><span class="math inline">\(\mathbf{q}\)</span> the attitude in the fixed world frame</li>
</ul>
<h2 id="sensors">Sensors</h2>
<p>The sensors at disposition are:</p>
<ul>
<li><p><strong>Accelerometer</strong>: It generates <span class="math inline">\(\mathbf{a_A}\)</span> a measurement of the total acceleration in the body frame referrential the drone is submitted to at a <strong>high</strong> sampling rate. If the object is submitted to no acceleration then the accelerometer measure the earth’s gravity field from. From that information, it could be possible to retrieve the attitude. Unfortunately, we are in a highly dynamic setting. Thus, it is possible when we can substract the drone’s acceleration from the thrust to the total acceleration. This would require to know exactly the force exerced by the rotors at each instant. In this work, we assume that doing that separation, while being theoretically possible, is too impractical. The measurements model is: <span class="math display">\[\mathbf{a_A}(t) = \mathbf{R}_{f2b}\{\mathbf{q}(t)\}\mathbf{a}(t) + \mathbf{a_A}^\epsilon\]</span> where the covariance matrix of the noise of the accelerometer is <span class="math inline">\({\mathbf{R}_{\mathbf{a_A}}}_{3 \times 3}\)</span> and <span class="math display">\[\mathbf{a_A}^\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{a_A}})\]</span>.</p></li>
<li><p><strong>Gyroscope</strong>:It generates <span class="math inline">\(\mathbf{\boldsymbol{\omega}_G}\)</span> a measurement of the angular velocity in the body frame of the drone at the last timestep at a <strong>high</strong> sampling rate. The measurement model is: <span class="math display">\[\mathbf{\boldsymbol{\omega}_G}(t) = \boldsymbol{\omega} + \mathbf{\boldsymbol{\omega}_G}^\epsilon\]</span> where the covariance matrix of the noise of the accelerometer is <span class="math inline">\({\mathbf{R}_{\mathbf{\boldsymbol{\omega}_G}}}_{3 \times 3}\)</span> and <span class="math display">\[\mathbf{\boldsymbol{\omega}_G}^\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{\boldsymbol{\omega}_G}})\]</span>.</p></li>
<li><p><strong>Position</strong>: It generates <span class="math inline">\(\mathbf{p_V}\)</span> a measurement of the current positionat a <strong>low</strong> sampling rate. This is usually provided by a <strong>Vicon</strong> (for indoor), <strong>GPS</strong>, a <strong>Tango</strong> or any other position sensor. The measurement model is: <span class="math display">\[\mathbf{p_V}(t) = \mathbf{p}(t) + \mathbf{p_V}^\epsilon\]</span> where the covariance matrix of the noise of the position is <span class="math inline">\({\mathbf{R}_{\mathbf{p_V}}}_{3 \times 3}\)</span> and <span class="math display">\[\mathbf{p_V}^\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{p_V}})\]</span>.</p></li>
<li><p><strong>Attitude</strong>: It generates <span class="math inline">\(\mathbf{q_V}\)</span> a measurement of the current attitute. This is usually provided in addition to the position by a <strong>Vicon</strong> or a <strong>Tango</strong> at a <strong>low</strong> sampling rate or the <strong>Magnemoter</strong> at a <strong>high</strong> sampling rate if the environment permit it (no high magnetic interference nearby like iron contamination). The magnetometer retrieve the attitude by assuming that the sensed magnetic field corresponds to the earth’s magnetic field. The measurement model is: <span class="math display">\[\mathbf{q_V}(t) = \mathbf{q}(t)*R2Q(\mathbf{q_V}^\epsilon)\]</span> where the <span class="math inline">\(3 \times 3\)</span> covariance matrix of the noise of the attitude in radian before being converted by <span class="math inline">\(R2Q\)</span> is <span class="math inline">\({\mathbf{R}_{\mathbf{q_V}}}_{3 \times 3}\)</span> and <span class="math display">\[\mathbf{q_V}^\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{q_V}})\]</span>.</p></li>
<li><p><strong>Optical Flow</strong>: A camera that keeps track of the movement by comparing the difference of the position of some reference points. By using a companion distance sensor, it is able to retrieve the difference between the two perspective and thus the change in angle and position. <span class="math display">\[\mathbf{dq_O}(t) = (\mathbf{q}(t-k)\mathbf{q}(t))*R2Q(\mathbf{dq_O}^\epsilon)\]</span> <span class="math display">\[\mathbf{dp_O}(t) = (\mathbf{p}(t) - \mathbf{p}(t-k)) + \mathbf{dp_O}^\epsilon\]</span></p></li>
</ul>
<p>where the <span class="math inline">\(3 \times 3\)</span> covariance matrix of the noise of the attitude variation in radian before being converted by <span class="math inline">\(R2Q\)</span> is <span class="math inline">\({\mathbf{R}_{\mathbf{dq_O}}}_{3 \times 3}\)</span> and <span class="math display">\[\mathbf{dq_O}^\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{dq_O}})\]</span> and the position variation covariance matrix <span class="math inline">\({\mathbf{R}_{\mathbf{dp_O}}}_{3 \times 3}\)</span> and <span class="math display">\[\mathbf{dp_O}^\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{dp_O}})\]</span>.</p>
<div class="figure">
<img src="opflow.jpg" alt="Optical flow from a moving drone" />
<p class="caption">Optical flow from a moving drone</p>
</div>
<p>The notable difference with the position or attitude sensor is that the optical flow sensor, like the IMU, only captures time variation, not absolute values.</p>
<ul>
<li><strong>Altimeter</strong>: An altimeter is a sensor that measure the altitude of the drone. For instance a LIDAR measure the time for the laser wave to reflect on a surface that is assumed to be the ground. A smart strategy is to only use the altimeter is oriented with a low angle to the earth, else you also have to account that angle in the estimation of the altitude. <span class="math display">\[z_A(t) = \sin(\text{pitch}(\mathbf{q(t)}))(\mathbf{p}(t).z + z_A^\epsilon)\]</span> <span class="math inline">\({R_{z_A}}_{3 \times 3}\)</span> and <span class="math display">\[z_A^\epsilon \sim \mathcal{N}(0, R_{z_A})\]</span></li>
</ul>
<div class="figure">
<img src="altimeter.jpg" alt="Rendering of the LIDAR laser of an altimeter" />
<p class="caption">Rendering of the LIDAR laser of an altimeter</p>
</div>
<p>Some sensors are more relevant indoor and some others outdoor:</p>
<ul>
<li><strong>Indoor</strong>: The sensors available indoor are the accelerometer, the gyroscope and the <strong>Vicon</strong>. The Vicon is a system composed of many sensors around a room that is able to track very accurately the position and orientation a mobile object. One issue with relying solely on the <strong>Vicon</strong> is that the sampling rate is low.</li>
</ul>
<div class="figure">
<img src="vicon.jpg" alt="A Vicon setup" />
<p class="caption">A Vicon setup</p>
</div>
<ul>
<li><strong>Outdoor</strong>: The sensors available outdoor are the accelerometer, the gyroscope, the magnetometer, two GPS, an optical flow and an altimeter.</li>
</ul>
<p>We assume that since the biases of the sensor could be known prior to the flight, the sensor have been calibrated and output measurements with no bias. Some filters like the <a href="https://dev.px4.io/en/tutorials/tuning_the_ecl_ekf.html">ekf2</a> of the px4 flight stack keep track of the sensor biases but this is a state augmentation that was not deemed worthwhile.</p>
<h3 id="control-inputs">Control inputs</h3>
<p>Observations from the control input are not strictly speaking measurements but input of the state-transition model. The IMU is a sensor, thus stricly speaking, its measurements are not control inputs. However, in the literature, it is standard to use its measurements as control inputs. One of the advantage is that the accelerometer measures acceleration and angular velocity, raw values close from the input we need in our state-transition. If we used a transformation of the thrust sent as command to the rotors, we would have to account for the rotors unprecision, the wind and other disturbances. Another advantage is that since the IMU has very high sampling rate, we can update very frequently the state with new transitions. The drawback is that the accelerometer is noisy. Fortunately, we can take into account the noise as a process model noise.</p>
<p>The control inputs at disposition are:</p>
<ul>
<li><strong>Acceleration</strong>: <span class="math inline">\(\mathbf{a_A}_t\)</span> from the acceloremeter</li>
<li><strong>Angular velocity</strong>: <span class="math inline">\(\mathbf{\boldsymbol{\omega}_G}_t\)</span> from the gyroscope.</li>
</ul>
<h3 id="model-dynamic">Model dynamic</h3>
<ul>
<li><span class="math inline">\(\mathbf{a}(t+1) = \mathbf{R}_{b2f}\{\mathbf{q}(t+1)\}(\mathbf{a_A}_t + \mathbf{a_A}^\epsilon_t)\)</span> where <span class="math inline">\(\mathbf{a}^\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}_{\mathbf{a}_t })\)</span></li>
<li><span class="math inline">\(\mathbf{v}(t+1) = \mathbf{v}(t) + \Delta t \mathbf{a}(t) + \mathbf{v}^\epsilon_t\)</span> where <span class="math inline">\(\mathbf{v}^\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}_{\mathbf{v}_t })\)</span></li>
<li><span class="math inline">\(\mathbf{p}(t+1) = \mathbf{p}(t) + \Delta t \mathbf{v}(t) + \mathbf{p}^\epsilon_t\)</span> where <span class="math inline">\(\mathbf{p}^\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}_{\mathbf{p}_t })\)</span></li>
<li><span class="math inline">\(\boldsymbol{\omega}(t+1) = \mathbf{\boldsymbol{\omega}_G}_t + \mathbf{\boldsymbol{\omega}_G}^\epsilon_t\)</span> where <span class="math inline">\(\mathbf{p}^\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}_{\mathbf{\boldsymbol{\omega}_G}_t })\)</span></li>
<li><span class="math inline">\(\mathbf{q}(t+1) = \mathbf{q}(t)*R2Q(\Delta t \boldsymbol{ \omega(t) })\)</span></li>
</ul>
<p>Note that in our model, <span class="math inline">\(\mathbf{q}(t+1)\)</span> must be known. Fortunately, as we will see later, our Rao-Blackwellized Particle Filter is conditionned under the attitude so it is known.</p>
<h2 id="state">State</h2>
<p>The time series of the variables of our dynamic model constitute a hidden markov chain. Indeed, the model is “memoryless” and depends only on the current state and a sampled transition.</p>
<p>States contain variables that enable us to keep track of some of those hidden variables which is our ultimate goal (for POSE <span class="math inline">\(\mathbf{p}\)</span> and <span class="math inline">\(\mathbf{q}\)</span>). States at time <span class="math inline">\(t\)</span> are denoted by <span class="math inline">\(\mathbf{x}_t\)</span>. Different filters require different state variables depending on their structure and assumptions.</p>
<h2 id="observation">Observation</h2>
<p>Observations are revealed variables conditionned under the variables of our dynamic model. Our ultimate goal is to deduce the states from the observations.</p>
<p>Observations contain the control input <span class="math inline">\(\mathbf{u}\)</span> and the measurements <span class="math inline">\(\mathbf{z}\)</span>.</p>
<p><span class="math display">\[\mathbf{y}_t = (\mathbf{z}_t, \mathbf{u}_t)^T = (\mathbf{p_V}_t, \mathbf{q_V}_t), ({t_C}_t, \mathbf{\boldsymbol{\omega}_C}_t))^T\]</span></p>
<h2 id="filtering-and-smoothing">Filtering and smoothing</h2>
<p><strong>Smoothing</strong> is the statistical task of finding the expectation of the state variable from the past history of observations and multiple observation variables ahead</p>
<p><span class="math display">\[\mathbb{E}[g(\mathbf{x}_{0:t}) | \mathbf{y}_{1:t+k}]\]</span></p>
<p>Which expand to,</p>
<p><span class="math display">\[\mathbb{E}[(\mathbf{p}_{0:t}, \mathbf{q}_{0:t}) | (\mathbf{z}_{1:t+k}, \mathbf{u}_{1:t+k})]\]</span></p>
<p><span class="math inline">\(k\)</span> is a contant and the first observation is <span class="math inline">\(y_1\)</span></p>
<p><strong>Filtering</strong> is a kind of smoothing where you only have at disposal the current observation variable (<span class="math inline">\(k=0\)</span>)</p>
<h2 id="complementary-filter">Complementary Filter</h2>
<p>The complementary filter is the simplest of all filter and very common to retrieve the attitude because of its low computational complexity. The gyroscope and accelerometer both provide a measurement that can help us to estimate the attitude. The gyroscope indeed gives us a noisy measurement of the angular velocity from which we can retrieve the new attitude from the past one by time integration: <span class="math inline">\(\mathbf{q}_t = \mathbf{q}_{t-1}*R2Q(\Delta t \mathbf{\omega})\)</span>.</p>
<p>This is commonly called “Dead reckoning”<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> and is prone to accumulation error, referred as drift. Indeed, like brownian motions, even if the process is unbiased, the variance grows with time. Reducing the noise cannot solve the issue entirely: even with extremely precise instruments, you are subject to floating point errors.</p>
<p>Fortunately, even though the accelerometer gives us a highly noisy (vibrations, wind, etc … ) measurement of the orientation, it is not subject to drift because it does not rely on accumulation. Indeed, if not subject to other accelerations, the accelerometer measures the gravity field orientation. Since this field is oriented toward earth, it is possible to retrieve the current rotation from that field and by extension the attitude. However, in the case of a drone, it is subject to continuous and signifiant acceleration and vibration. Hence, the assumption that we retrieve the gravity field directly is wrong. Nevertheless, We could solve this by substracting the acceleration deduced from the thrust control input. It is unpractical so this approach is not pursued in this work, but understanding this filter is still useful.</p>
<p>The idea of the filter itself is to combine the precise “short-term” measurements of the gyroscope subject to drift with the “long-term” measurements of the accelerometer.</p>
<h3 id="state-1">State</h3>
<p>This filter is very simple and it is only needed to store as a state the last estimated attitude along with its timestamp (to calculate <span class="math inline">\(\Delta t\)</span>). <span class="math display">\[\mathbf{x}_t = \mathbf{q}_t\]</span> <span class="math display">\[\hat{\mathbf{q}}_{t+1} = \alpha (\hat{\mathbf{q}}_t + \Delta t \mathbf{\omega}_t) + (1 - \alpha) {\mathbf{q_A}}_{t+1}\]</span> <span class="math inline">\(\alpha \in [0, 1]\)</span>. Usually, <span class="math inline">\(\alpha\)</span> is set to a high-value like <span class="math inline">\(0.98\)</span>. It is very intuitive to see why this should approximately “work”, the data from the accelerometer continuously correct the drift from the gyroscope.</p>
<div class="figure">
<img src="ocf.png" alt="CF Graph generated by scala-flow" />
<p class="caption">CF Graph generated by scala-flow</p>
</div>
<p>Figure 9 is the plot of the distance from the true quaternion after 15s of an arbitrary trajectory when <span class="math inline">\(\alpha = 1.0\)</span> meaning that the accelerometer does not correct the drift.</p>
<div class="figure">
<embed src="cf100.pdf" />
<p class="caption">CF with alpha = 1.0</p>
</div>
<p>Figure 10 is that same trajectory with <span class="math inline">\(\alpha = 0.98\)</span>.</p>
<div class="figure">
<embed src="cf098.pdf" />
<p class="caption">CF with alpha = 0.98</p>
</div>
<h2 id="asynchronous-augmented-complementary-filter">Asynchronous Augmented Complementary Filter</h2>
<p>As explained previously, in this highly-dynamic setting, combining the gyroscope and the accelerometer to retrieve the attitude is not satisfactory. However, we can reuse the intuition from the complementary filter, which is to combine precise but drifting short-term measurement to other measurements that do not suffer from drift. This enable a simple and computionally inexpensive novel filter that we will be able to use later as a baseline. In this case, the short-term measurements are the acceleration and angular velocity from the IMU, and the non drifting measurements come from the Vicon.</p>
<p>We will also add the property that the data from the sensors are asynchronous. This is a consequence of the sensors having different sampling rate.</p>
<ul>
<li><p><strong>IMU</strong> update <span class="math display">\[\mathbf{v}_t = \mathbf{v}_{t-1} + \Delta t_v \mathbf{a_A}_t\]</span> <span class="math display">\[\boldsymbol{\omega}_t = \boldsymbol{\mathbf{\omega_G}}_t\]</span> <span class="math display">\[\mathbf{p}_t = \mathbf{p}_{t-1} + \Delta t \mathbf{v}_{t-1}\]</span> <span class="math display">\[\mathbf{q}_t = \mathbf{q}_{t-1}R2Q(\Delta t \boldsymbol{\omega}_{t-1})\]</span></p></li>
<li><p><strong>Vicon</strong> update <span class="math display">\[\mathbf{p}_t = \alpha \mathbf{p_V} + (1 - \alpha) (\mathbf{p}_{t-1} + \Delta t \mathbf{v}_{t-1})\]</span> <span class="math display">\[\mathbf{q}_t = \alpha \mathbf{q_V} + (1 - \alpha) (\mathbf{q}_{t-1}R2Q(\Delta t \boldsymbol{\omega}_{t-1}))\]</span></p></li>
</ul>
<h3 id="state-2">State</h3>
<p>The state has to be more complex because the filter estimate now both the position and the attitude. Furthermore, because of asynchronousity, we have to store the last angular velocity, the last linear velocity, and the last time the linear velocity has been updated (to retrieve <span class="math inline">\(\Delta t_v = t - t_a\)</span> where <span class="math inline">\(t_a\)</span> is the last time we had an update from the accelerometer).</p>
<p><span class="math display">\[\mathbf{x}_t = (\mathbf{p}_t, \mathbf{q}_t, \boldsymbol{\omega}_t, \mathbf{a}_t, t_a)\]</span></p>
<div class="figure">
<img src="acf.png" alt="AACF Graph generated by scala-flow" />
<p class="caption">AACF Graph generated by scala-flow</p>
</div>
<h2 id="kalman-filter">Kalman Filter</h2>
<h3 id="bayesian-inference">Bayesian inference</h3>
<p>Bayesian inference is a method of statistical inference in which Bayes’ theorem is used to update the probability for a hypothesis as more evidence or information becomes available. In this Bayes setting, the prior is the estimated distribution of the previous state at time <span class="math inline">\(t-1\)</span>, the likelihood correspond to the likelihood of getting the new data from the sensor given the prior and finally, the posterior is the updated estimated distribution.</p>
<h3 id="model-1">Model</h3>
<p>The kalman filter requires that both the model process and the measurement process are <strong>linear gaussian</strong>. Linear gaussian processes are of the form: <span class="math display">\[\mathbf{x}_t = f(\mathbf{x}_{t-1}) + \mathbf{w}_t\]</span> where <span class="math inline">\(f\)</span> is a linear function and <span class="math inline">\(\mathbf{w}_t\)</span> a gaussian process: it is sampled from an arbitrary gaussian distribution.</p>
<p>The Kalman filter is a direct application of bayesian inference. It combines the prediction of the distribution given the estimated prior state and the state-transition model.</p>
<p><span class="math display">\[\mathbf{x}_t = \mathbf{F}_t \mathbf{x}_{t-1} + \mathbf{B}_t \mathbf{u}_t + \mathbf{w}_t \]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{x}_t\)</span> the state</li>
<li><span class="math inline">\(\mathbf{F}_t\)</span> the state transition model</li>
<li><span class="math inline">\(\mathbf{B}_t\)</span> the control-input model</li>
<li><span class="math inline">\(\mathbf{u}_t\)</span> the control vector</li>
<li><span class="math inline">\(\mathbf{w}_t\)</span> process noise drawn from <span class="math inline">\(\mathbf{w}_t \sim N(0, \mathbf{Q}_k)\)</span></li>
</ul>
<p>and the estimated distribution given the data coming from the sensors.</p>
<p><span class="math display">\[\mathbf{y}_t = \mathbf{H}_t \mathbf{x}_{t}  + \mathbf{v}_t \]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{y}_t\)</span> measurements</li>
<li><span class="math inline">\(\mathbf{H}_t\)</span> the state to measurement matrix</li>
<li><span class="math inline">\(\mathbf{w}_t\)</span> measurement noise drawn from <span class="math inline">\(\mathbf{w}_t \sim N(0, \mathbf{R}_k)\)</span></li>
</ul>
<p>Because, both the model process and the sensor process are assumed to be linear gaussian, we can combine them into a gaussian distribution. Indeed, the product of two gaussians is gaussian.</p>
<p><span class="math display">\[P(\mathbf{x}_{t}) \propto P(\mathbf{x}^{-}_{t}|\mathbf{x}_{t-1}) \cdot P(\mathbf{x}_t | \mathbf{y}_t )\]</span> <span class="math display">\[\mathcal{N}(\mathbf{x}_{t}) \propto \mathcal{N}(\mathbf{x}^{-}_{t}|\mathbf{x}_{t-1}) \cdot \mathcal{N}(\mathbf{x}_t | \mathbf{y}_t )\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}^{-}_{t}\)</span> is the predicted state from the previous state and the state-transition model.</p>
<p>The kalman filter keep track of the parameters of that gaussian: the mean state and the covariance of the state which represent the uncertainty about our last prediction. The mean of that distribution is also the best current state estimation of the filter.</p>
<p>By keeping track of the uncertainty, we can optimally combine the normals by knowing what importance to give to the difference between the expected sensor data and the actual sensor data. That factor is the Kalman gain.</p>
<ul>
<li><strong>predict</strong>:
<ul>
<li>predicted <strong>state</strong>: <span class="math inline">\(\hat{\mathbf{x}}^{-}_t = \mathbf{F}_t \mathbf{x}_{t-1} + \mathbf{B}_t \mathbf{u}_t\)</span></li>
<li>predicted <strong>covariance</strong>: <span class="math inline">\(\mathbf{\Sigma}^{-}_t = \mathbf{F}_{t-1} \mathbf{\Sigma}^{-}_{t-1} \mathbf{F}_{t-1}^T + \mathbf{Q}_t\)</span></li>
</ul></li>
<li><strong>update</strong>:
<ul>
<li>predicted <strong>measurements</strong>: <span class="math inline">\(\hat{\mathbf{z}} = \mathbf{H}_t \hat{\mathbf{x}}^{-}_t\)</span></li>
<li><strong>innovation</strong>: <span class="math inline">\((\mathbf{z}_t - \hat{\mathbf{z}})\)</span><br />
</li>
<li><strong>innovation covariance</strong>: <span class="math inline">\(\mathbf{S} = \mathbf{H}_t \mathbf{\Sigma}^{-}_t \mathbf{H}_t^T + \mathbf{R}_t\)</span><br />
</li>
<li>optimal <strong>kalman gain</strong>: <span class="math inline">\(\mathbf{K} = \mathbf{\Sigma}^{-}_t \mathbf{H}_t^T \mathbf{S}^{-1}\)</span></li>
<li>updated <strong>state</strong>: <span class="math inline">\(\mathbf{\Sigma}_t = \mathbf{\Sigma}^-_t + \mathbf{K} \mathbf{S} \mathbf{K}^T\)</span></li>
<li>updated <strong>covariance</strong>: <span class="math inline">\(\hat{\mathbf{x}}_t = \hat{\mathbf{x}}^{-}_t + \mathbf{K}(\mathbf{z}_t - \hat{\mathbf{z}})\)</span></li>
</ul></li>
</ul>
<h2 id="asynchronous-kalman-filter">Asynchronous Kalman Filter</h2>
<p>It is not necessary to apply the full kalman update at each measurement. Indeed, <span class="math inline">\(\mathbf{H}\)</span> can be sliced to correspond to the measurements currently available.</p>
<p>To be truly asynchronous, you also have to account for the different sampling rates. There is two cases :</p>
<ul>
<li>The required data for the update step (the control inputs) can arrive multiple time before any of the data of the update step (the measurements) occur.</li>
<li>Inversely, it is possible that the measurements occur at a higher sampling rate than the control inputs.</li>
</ul>
<p>The strategy chosen here is as follow:</p>
<ol style="list-style-type: decimal">
<li>Multiple prediction steps without any update step may happen without making the algorithm inconsistent.</li>
<li>An update is <strong>always</strong> immediatly preceded by a prediction step. This is a consequence of the requirement that the innovation must measure the difference between the predicted measurement from the state at the exact current time and the measurements. Thus, if the measurements are not synchronised with the control inputs, use the most likely control input for the prediction step, which might result in simply repeating them. Repeating the last control input was the method used for the accelerometer and the gyroscope data as control input.</li>
</ol>
<h2 id="extended-kalman-filters">Extended Kalman Filters</h2>
<p>In the previous section, we have shown that the Kalman Filter is only applicable when both the process model and the measurement model are linear gaussian process. This has two aspects:</p>
<ul>
<li>The noise of the measurements and of the state-transition must be gaussian</li>
<li>The state-transition function and the measurement to state function must be linear.</li>
</ul>
<p>Furthermore, it is provable that kalman filters are optimal linear filters.</p>
<p>However, in our context, one component of the state, the attitude, is intrisically non-linear. Indeed, rotations and attitudes belong to <span class="math inline">\(SO(3)\)</span> which is not a vector space. Therefore, we cannot use <em>vanilla</em> kalman filters. The filters that we present thereafter relax those requirements.</p>
<p>One example of such extension is the extended kalman filter (EKF) that we will present here. The EKF relax the linearity requirement by using differentiation tocalculate an approximation of the first order of the required linear functions. Our state transition function and measurement function can now be expressed in the free forms <span class="math inline">\(f(\mathbf{x}_t)\)</span> and <span class="math inline">\(h(\mathbf{x}_t)\)</span> and we define the matrix <span class="math inline">\(\mathbf{F}_t\)</span> and <span class="math inline">\(\mathbf{H}_t\)</span> as their jacobian.</p>
<p><span class="math display">\[{\mathbf{F}_t}_{10 \times 10} = \left . \frac{\partial f}{\partial \mathbf{x} } \right \vert _{\hat{\mathbf{x}}_{t-1},\mathbf{u}_{t-1}}\]</span></p>
<p><span class="math display">\[{\mathbf{H}_t}_{7 \times 7} = \left . \frac{\partial h}{\partial \mathbf{x} } \right \vert _{\hat{\mathbf{x}}_{t}}\]</span></p>
<ul>
<li><strong>predict</strong>:
<ul>
<li>predicted <strong>state</strong>: <span class="math inline">\(\hat{\mathbf{x}}^{-}_t = f(\mathbf{x}_{t-1}) + \mathbf{B}_t \mathbf{u}_t\)</span></li>
<li>predicted <strong>covariance</strong>: <span class="math inline">\(\mathbf{\Sigma}^{-}_t = \mathbf{F}_{t-1} \mathbf{\Sigma}^{-}_{t-1} \mathbf{F}_{t-1}^T + \mathbf{Q}_t\)</span></li>
</ul></li>
<li><strong>update</strong>:
<ul>
<li>predicted <strong>measurements</strong>: <span class="math inline">\(\hat{\mathbf{z}} = h(\hat{\mathbf{x}}^{-}_t)\)</span></li>
<li><strong>innovation</strong>: <span class="math inline">\((\mathbf{z}_t - \hat{\mathbf{z}})\)</span><br />
</li>
<li><strong>innovation covariance</strong>: <span class="math inline">\(\mathbf{S} = \mathbf{H}_t \mathbf{\Sigma}^{-}_t \mathbf{H}_t^T + \mathbf{R}_t\)</span><br />
</li>
<li>optimal <strong>kalman gain</strong>: <span class="math inline">\(\mathbf{K} = \mathbf{\Sigma}^{-}_t \mathbf{H}_t^T \mathbf{S}^{-1}\)</span></li>
<li>updated <strong>state</strong>: <span class="math inline">\(\mathbf{\Sigma}_t = \mathbf{\Sigma}^-_t + \mathbf{K} \mathbf{S} \mathbf{K}^T\)</span></li>
<li>updated <strong>covariance</strong>: <span class="math inline">\(\hat{\mathbf{x}}_t = \hat{\mathbf{x}}^{-}_t + \mathbf{K}(\mathbf{z}_t - \hat{\mathbf{z}})\)</span></li>
</ul></li>
</ul>
<h3 id="ekf-for-pose">EKF for POSE</h3>
<h4 id="state-3">State</h4>
<p>For the EKF, we are gonna use the following state:</p>
<p><span class="math display">\[\mathbf{x}_t = (\mathbf{v}_t, \mathbf{p}_t, \mathbf{q}_t)^T\]</span></p>
<p>Initial state <span class="math inline">\(\mathbf{x}_0\)</span> at <span class="math inline">\((\mathbf{0}, \mathbf{0}, (1, 0, 0, 0))\)</span></p>
<h4 id="indoor-measurements-model">Indoor Measurements model</h4>
<ol style="list-style-type: decimal">
<li>Position: <span class="math display">\[\mathbf{p_V}(t) = \mathbf{p}(t)^{(i)} + \mathbf{p_V}^\epsilon_t\]</span> where <span class="math inline">\(\mathbf{p_V}^\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{p_V}_t })\)</span></li>
<li>Attitude: <span class="math display">\[\mathbf{q_V}(t) = \mathbf{q}(t)^{(i)}*R2Q(\mathbf{q_V}^\epsilon_t)\]</span> where <span class="math inline">\(\mathbf{q_V}^\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{q_V}_t })\)</span></li>
</ol>
<h4 id="kalman-prediction">Kalman prediction</h4>
<p>The model dynamic defines the following model, state-transition function <span class="math inline">\(f(\mathbf{x}, \mathbf{u})\)</span> and process noise <span class="math inline">\(\mathbf{w}\)</span> with covariance matrix <span class="math inline">\(\mathbf{Q}\)</span></p>
<p><span class="math display">\[\mathbf{x}_t = f(\mathbf{x}_{t-1}, \mathbf{u}_t) + \mathbf{w}_t\]</span></p>
<p><span class="math display">\[f((\mathbf{v}, \mathbf{p}, \mathbf{q}), (\mathbf{a_A}, \mathbf{\boldsymbol{\omega}_G})) = \left( \begin{array}{c}
\mathbf{v} + \Delta t \mathbf{R}_{b2f}\{\mathbf{q}_{t-1}\} \mathbf{a} \\
\mathbf{p} + \Delta t \mathbf{v} \\
\mathbf{q}*R2Q({\Delta t} \boldsymbol{\omega}_G)
\end{array} \right)\]</span></p>
<p>Now, we need to derive the jacobian of <span class="math inline">\(f\)</span>. We will use sagemath to retrieve the 28 relevant different partial derivatives of <span class="math inline">\(q\)</span>.</p>
<p><span class="math display">\[{\mathbf{F}_t}_{10 \times 10} = \left . \frac{\partial f}{\partial \mathbf{x} } \right \vert _{\hat{\mathbf{x}}_{t-1},\mathbf{u}_{t-1}}\]</span></p>
<p><span class="math display">\[\hat{\mathbf{x}}^{-(i)}_t = f(\mathbf{x}^{(i)}_{t-1}, \mathbf{u}_t)\]</span> <span class="math display">\[\mathbf{\Sigma}^{-(i)}_t = \mathbf{F}_{t-1} \mathbf{\Sigma}^{-(i)}_{t-1}  \mathbf{F}_{t-1}^T + \mathbf{Q}_t\]</span></p>
<h4 id="kalman-measurements-update">Kalman measurements update</h4>
<p><span class="math display">\[\mathbf{z}_t = h(\mathbf{x}_t) + \mathbf{v}_t\]</span></p>
<p>The <a href="#measurements-model">measurement model</a> defines <span class="math inline">\(h(\mathbf{x})\)</span></p>
<p><span class="math display">\[\left( \begin{array}{c}
\mathbf{p_V}\\
\mathbf{q_V}\\
\end{array} \right) = h((\mathbf{v}, \mathbf{p}, \mathbf{q})) = \left( \begin{array}{c}
\mathbf{p}\\
\mathbf{q}\\
\end{array} \right)\]</span></p>
<p>The only complex partial derivatives to calculate are the one of the acceleration, because they have to be rotated first. Once again, we use sagemath: <span class="math inline">\(\mathbf{H_a}\)</span> is defined by the script in the appendix B.</p>
<p><span class="math display">\[{\mathbf{H}_t}_{10 \times 7} = \left . \frac{\partial h}{\partial \mathbf{x} } \right \vert _{\hat{\mathbf{x}}_{t}} = \left( \begin{array}{ccc}
\mathbf{0}_{3 \times 3} &amp; &amp; \\
&amp; \mathbf{I}_{3 \times 3} &amp; \\
&amp; &amp; \mathbf{I}_{4 \times 4}\\
\end{array} \right)\]</span></p>
<p><span class="math display">\[{\mathbf{R}_t}_{7 \times 7} = 
\left( \begin{array}{cc}
\mathbf{R}_{\mathbf{p_V}} &amp; \\
&amp;  {\mathbf{R}_{\mathbf{q_V}}}_{4 \times 4}\\
\end{array} \right)\]</span></p>
<p><span class="math inline">\(\mathbf{R}_{\mathbf{q_V}}\)</span> has to be <span class="math inline">\(4 \times 4\)</span> and has to represent the covariance of the quaternion. This covariance matrix thus to be linearized first around the current point of the quaternion. It is then possible to use the corresponding covariance of the linearized form.</p>
<h4 id="kalman-update">Kalman update</h4>
<p><span class="math display">\[\mathbf{S} = \mathbf{H}_t \mathbf{\Sigma}^{-}_t \mathbf{H}_t^T + \mathbf{R}_t\]</span> <span class="math display">\[\hat{\mathbf{z}} = h(\hat{\mathbf{x}}^{-}_t)\]</span> <span class="math display">\[\mathbf{K} = \mathbf{\Sigma}^{-}_t \mathbf{H}_t^T \mathbf{S}^{-1}\]</span> <span class="math display">\[\mathbf{\Sigma}_t = \mathbf{\Sigma}^-_t + \mathbf{K} \mathbf{S} \mathbf{K}^T\]</span> <span class="math display">\[\hat{\mathbf{x}}_t = \hat{\mathbf{x}}^{-}_t + \mathbf{K}(\mathbf{z}_t - \hat{\mathbf{z}})\]</span></p>
<h2 id="f-partial-derivatives">F partial derivatives</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">Q.<span class="op">&lt;</span>i,j,k<span class="op">&gt;</span> <span class="op">=</span> QuaternionAlgebra(SR, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)

var(<span class="st">&#39;q0, q1, q2, q3&#39;</span>)
var(<span class="st">&#39;dt&#39;</span>)
var(<span class="st">&#39;wx, wy, wz&#39;</span>)

q <span class="op">=</span> q0 <span class="op">+</span> q1<span class="op">*</span>i <span class="op">+</span> q2<span class="op">*</span>j <span class="op">+</span> q3<span class="op">*</span>k

w <span class="op">=</span> vector([wx, wy, wz])<span class="op">*</span>dt
w_norm <span class="op">=</span> sqrt(w[<span class="dv">0</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span> w[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span> w[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span>)
ang <span class="op">=</span> w_norm<span class="op">/</span><span class="dv">2</span>
w_normalized <span class="op">=</span> w<span class="op">/</span>w_norm
sin2 <span class="op">=</span> sin(ang)
qd <span class="op">=</span> cos(ang) <span class="op">+</span> w_normalized[<span class="dv">0</span>]<span class="op">*</span>sin2<span class="op">*</span>i <span class="op">+</span> w_normalized[<span class="dv">1</span>]<span class="op">*</span>sin2<span class="op">*</span>j <span class="op">+</span> w_normalized[<span class="dv">2</span>]<span class="op">*</span>sin2<span class="op">*</span>k

nq <span class="op">=</span> q<span class="op">*</span>qd

v <span class="op">=</span> vector(nq.coefficient_tuple())

<span class="cf">for</span> sym <span class="kw">in</span> [wx, wy, wz, q0, q1, q2, q3]:
    d <span class="op">=</span> diff(v, sym)
    exps <span class="op">=</span> <span class="bu">map</span>(<span class="kw">lambda</span> x: x.canonicalize_radical().full_simplify(), d)
    <span class="cf">for</span> i, e <span class="kw">in</span> <span class="bu">enumerate</span>(exps):
        <span class="bu">print</span>(sym, i, e) 
        </code></pre></div>
<h2 id="unscented-kalman-filters">Unscented Kalman Filters</h2>
<p>The EKF has 3 flaws in our case:</p>
<ul>
<li>The linearization gives an approximate form which result in approximation errors</li>
<li>The prediction step of the EKF assume that the linearized form of the transformation can capture all the information needed to apply the transformation to the gaussian distribution pre-transformation. Unfortunately, this is only true near the region of the mean. The transformation of the tail of the gaussian distribution may need to be very different.</li>
<li>It attempts to define a gaussian covariance matrix for the attitude quaternion. This does not make sense because it does not account for the requirement of the quaternion being in a 4 dimensional unit sphere.</li>
</ul>
<p>The Unscented Kalman Filter (UKF) does not suffer from the two first flaws, but it is more computationally expensive as it requires a cholesky factorisation that grows exponentially in complexity with the number of dimensions.</p>
<p>Indeed, the UKF applies an unscented transformation to sigma points of the current approximated distribution. The sigma points are the representative of the distribution. They are chosen 1 standard deviation plus and minus of the mean in each dimension. This means that we no longer only keep track of the mean and its covariance but also the effect of the transformation away from the mean. 1 standard deviation away is a good compromise because around 70% of the distribution is included between the bounds delimited by the two standard deviations.</p>
<p>In one dimension, the square root of the variance is enough. In N-dimension, you must use the cholesky decomposition of the covariance matrix. The cholesky decomposition find the matrix <span class="math inline">\(L\)</span> such that <span class="math inline">\(\Sigma = LL^t\)</span>.</p>
<p>The implementation of an UKF still suffer greatly from quaternion not belonging to a vector space. The approach taken by <span class="citation">(Edgar, n.d.)</span> is to use the error quaternion defined by <span class="math inline">\(\mathbf{e}_i = \mathbf{q}_i\bar{\mathbf{q}}\)</span>. This approach has the benefit that similar quaternion differences result in similar error. But apart from that, it does not have any profound justification. We must compute a sound average weighted quaternion of all sigma points. An algorithm is described in the following section.</p>
<h3 id="average-quaternion">Average quaternion</h3>
<p>Unfortunately, the average of quaternions components <span class="math inline">\(\frac{1}{N} \sum q_i\)</span> or <em>barycentric</em> mean is unsound: Indeed, attitude do not belong to a vector space but a homogenous Riemannian manifold (the four dimensional unit sphere). To convince yourself of the unsoundness of the <em>barycentric</em> mean, see that the addition and barycentric mean of two unit quaternion is not necessarily an unit quaternion (<span class="math inline">\((1, 0, 0, 0)\)</span> and <span class="math inline">\((-1, 0, 0, 0)\)</span> for instance. Furthermore, angle being periodic, the <em>barycentric</em> mean of a quaternion with angle <span class="math inline">\(-178^\circ\)</span> and another with same body-axis and angle <span class="math inline">\(180^\circ\)</span> gives <span class="math inline">\(1^\circ\)</span> instead of the expected <span class="math inline">\(-179^\circ\)</span>.</p>
<p>To calculate the average quaternion, we use an algorithm which minimize a metric that correspond to the weighted attitude difference to the average, namely the weighted sum of the squared Frobenius norms of attitude matrix differences. <span class="math display">\[\bar{\mathbf{q}} = arg \min_{q \in \mathbb{S}^3} \sum w_i \| A(\mathbf{q}) - A(\mathbf{q}_i) \|^2_F\]</span></p>
<p>where <span class="math inline">\(\mathbb{S}^3\)</span> denotes the unit sphere.</p>
<p>The attitude matrix <span class="math inline">\(A(\mathbf{q})\)</span> and its corresponding Frobenius norm have been described in the quaternion section.</p>
<h3 id="intuition">Intuition</h3>
<p>The intuition of keeping track of multiple representative of the distribution is exactly the approach taken by the particle filter. The particle filter has the advantage that the distribution is never transformed back to a gaussian so there is less assumption made about the noise and the transformation. It is only required to be able to calculate the expectation from a weighted set of particles.</p>
<h2 id="particle-filter">Particle Filter</h2>
<p>Particle filters are sequential monte carlo methods. Like all monte carlo method, they rely on repeated sampling for estimation of a distribution.</p>
<div class="figure">
<img src="mc.gif" alt="Monte carlo estimation of pi" />
<p class="caption">Monte carlo estimation of pi</p>
</div>
<p>The particle filter itself a weighted particle representation of the posterior:</p>
<p><span class="math display">\[p(\mathbf{x}) = \sum w^{(i)}\delta(\mathbf{x} - \mathbf{x}^{(i)})\]</span> where <span class="math inline">\(\delta\)</span> is the dirac delta function. The dirac delta function is zero everywhere except at zero, with an integral of one over the entire real line. It represents here the ideal probability density of a particle.</p>
<h3 id="importance-sampling">Importance sampling</h3>
<p>The weights here come from importance sampling. It represents the fact that each particle does not represent equally the distribution. Importance sampling enables to use sampling from another distribution to estimate properties from the target distribution of interest. In most cases, it can be used to focus sampling on the region of interest. But in our case, it enables us to reweight particles based on their likelihood from the measurements.</p>
<p>Importance sampling is based on the identity:</p>
<p><span class="math display">\[\mathcal{E}[\mathbf{g}(\mathbf{x}) | \mathbf{y}_{1:T}] = \int \mathbf{g}(\mathbf{x})p(\mathbf{x}|\mathbf{y}_{1:T}d\mathbf{x})\]</span></p>
<h3 id="sequential-importnce-samplng">Sequential Importnce Samplng</h3>
<p>** TODO **</p>
<p>Particle filters are very computionally expensive and that is why their usage is not very popular currently for low-powered embedded systems like drones (But they are used in Avionics). Using accelerated hardware is one way to enable them on drones.</p>
<h3 id="resampling">Resampling</h3>
<p>When the number of effective particles is too low (<span class="math inline">\(N/10\)</span>), we apply systematic resampling. The idea behind resampling is simple. The distribution is represented by a number of particles with different weights. As time goes, the repartition of weights degenerate. A large subset of particles ends up having negligible weight which make them irrelevant. In the most extreme case, one particle represents the whole distribution. To avoid that degeneration, when the weights are too unbalanced, we resample from the weights distribution: pick N times among the particle and assign them a weight of <span class="math inline">\(1/N\)</span>, each pick has odd <span class="math inline">\(w_i\)</span> to pick the particle <span class="math inline">\(p_i\)</span>. Thus, some particles with large weights are splitted up into smaller clone particle and others with small weight are never picked. This process is similar to evolution, at each generation, the most promising branch survive and replicate while the less promising die off.</p>
<p>A popular method for resampling is systematic sampling as described by <span class="citation">(Doucet and Johansen <a href="#ref-doucet_tutorial_2009">2009</a>)</span>:</p>
<p>Sample <span class="math inline">\(U_1 \sim \mathcal{U} [0, \frac{1}{N} ]\)</span> and define <span class="math inline">\(U_i = U_1 + \frac{i-1 }{N}\)</span> for <span class="math inline">\(i = 2, \ldots, N\)</span></p>
<h2 id="rao-blackwellized-particle-filter">Rao-Blackwellized Particle Filter</h2>
<h3 id="introduction-1">Introduction</h3>
<p>Compared to a plain PF, RPBF leverage the linearity of some components of the state by assuming our model gaussian conditionned on a latent variable: Given the attitude <span class="math inline">\(q_t\)</span>, our model is linear. This is where RPBF shines: We use particle filtering to estimate our latent variable, the attitude, and we use the optimal kalman filter to estimate the state variable.</p>
<p>This main inspiration from this approach is <span class="citation">(Vernaza and Lee <a href="#ref-vernaza_rao-blackwellized_2006">2006</a>)</span>. However, it differs by:</p>
<ul>
<li>adapt the filter to drones by taking into account that the system is too dynamic for assuming that the accelerometer simply output the gravity vector. This is solved by augmenting the state with the acceleration as shown later.</li>
<li>not use measurements of the IMU as control inputs (this is usually used for wheeled vehicles because of the drift from the wheels) but have both control inputs and measurements.</li>
<li>add an attitude sensor.</li>
</ul>
<p>We introduce the latent variable <span class="math inline">\(\boldsymbol{\theta}\)</span></p>
<p>The latent variable <span class="math inline">\(\boldsymbol{\theta}\)</span> has for sole component the attitude: <span class="math display">\[\boldsymbol{\theta} = (\mathbf{q})\]</span></p>
<p><span class="math inline">\(q_t\)</span> is estimated from the product of the attitude of all particles <span class="math inline">\(\mathbf{\theta^{(i)}} = \mathbf{q}^{(i)}_t\)</span> as the “average” quaternion <span class="math inline">\(\mathbf{q}_t = avgQuat(\mathbf{q}^n_t)\)</span>. <span class="math inline">\(x^n\)</span> designates the product of all n arbitrary particle.</p>
<p>The weight definition is:</p>
<p><span class="math display">\[w^{(i)}_t = \frac{p(\boldsymbol{\theta}^{(i)}_{0:t} | \mathbf{y}_{1:t})}{\pi(\boldsymbol{\theta}^{(i)}_{0:t} | \mathbf{y}_{1:t})}\]</span></p>
<p>From the definition, it is proovable that:</p>
<p><span class="math display">\[w^{(i)}_t \propto \frac{p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1}, \mathbf{y}_{1:t-1})p(\boldsymbol{\theta}^{(i)}_t | \boldsymbol{\theta}^{(i)}_{t-1})}{\pi(\boldsymbol{\theta}^{(i)}_t | \boldsymbol{\theta}^{(i)}_{1:t-1}, \mathbf{y}_{1:t})} w^{(i)}_{t-1}\]</span></p>
<p>We choose the dynamic of the model as the importance distribution:</p>
<p><span class="math display">\[\pi(\boldsymbol{\theta}^{(i)}_t | \boldsymbol{\theta}^{(i)}_{1:t-1}, \mathbf{y}_{1:t}) = p(\boldsymbol{\theta}^{(i)}_t | \boldsymbol{\theta}^{(i)}_{t-1}) \]</span></p>
<p>Hence,</p>
<p><span class="math display">\[w^{(i)}_t \propto p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1}, \mathbf{y}_{1:t-1}) w^{(i)}_{t-1}\]</span></p>
<p>We then sum all <span class="math inline">\(w^{(i)}_t\)</span> to find the normalization constant and retrieve the actual <span class="math inline">\(w^{(i)}_t\)</span></p>
<h3 id="state-4">State</h3>
<p><span class="math display">\[\mathbf{x}_t = (\mathbf{v}_t, \mathbf{p}_t)^T\]</span></p>
<p>Initial state <span class="math inline">\(\mathbf{x}_0 = (\mathbf{0}, \mathbf{0}, \mathbf{0})\)</span></p>
<p>Initial covariance matrix <span class="math inline">\(\mathbf{\Sigma}_{6 \times 6} = \epsilon \mathbf{I}_{6 \times 6}\)</span></p>
<h3 id="latent-variable">Latent variable</h3>
<p><span class="math display">\[\mathbf{q}^{(i)}_{t+1} = \mathbf{q}^{(i)}_t*R2Q({\Delta t} (\mathbf{\boldsymbol{\omega}_G}_t+\mathbf{\boldsymbol{\omega}_G}^\epsilon_t))\]</span></p>
<p><span class="math inline">\(\mathbf{\boldsymbol{\omega}_G}^\epsilon_t\)</span> represents the error from the control input and is sampled from <span class="math inline">\(\mathbf{\boldsymbol{\omega}_G}^\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{\boldsymbol{\omega}_G}_t })\)</span></p>
<p>Initial attitude <span class="math inline">\(\mathbf{q_0}\)</span> is sampled such that the drone pitch and roll are none (parralel to the ground) but the yaw is unknown and uniformly distributed.</p>
<p>Note that <span class="math inline">\(\mathbf{q}(t+1)\)</span> is known in the <a href="#model-dynamic">model dynamic</a> because the model is conditionned under <span class="math inline">\(\boldsymbol{\theta}^{(i)}_{t+1}\)</span>.</p>
<h3 id="indoor-measurement-model">Indoor Measurement model</h3>
<ol style="list-style-type: decimal">
<li>Position: <span class="math display">\[\mathbf{p_V}(t) = \mathbf{p}(t)^{(i)} + \mathbf{p_V}^\epsilon_t\]</span> where <span class="math inline">\(\mathbf{p_V}^\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{p_V}_t })\)</span></li>
<li>Attitude: <span class="math display">\[\mathbf{q_V}(t) = \mathbf{q}(t)^{(i)}*R2Q(\mathbf{q_V}^\epsilon_t)\]</span> where <span class="math inline">\(\mathbf{q_V}^\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{q_V}_t })\)</span></li>
</ol>
<h3 id="kalman-prediction-1">Kalman prediction</h3>
<p>The model dynamics define the following model, state-transition matrix <span class="math inline">\(\mathbf{F}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span>, the control-input matrix <span class="math inline">\(\mathbf{B}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span>, the process noise <span class="math inline">\(\mathbf{w}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span> for the Kalman filter and its covariance <span class="math inline">\(\mathbf{Q}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span></p>
<p><span class="math display">\[\mathbf{x}_t = \mathbf{F}_t\{\boldsymbol{\theta}^{(i)}_t\} \mathbf{x}_{t-1} + \mathbf{B}_t\{\boldsymbol{\theta}^{(i)}_t\} \mathbf{u}_t + \mathbf{w}_t\{\boldsymbol{\theta}^{(i)}_t\}\]</span></p>
<p><span class="math display">\[\mathbf{F}_t\{\boldsymbol{\theta}^{(i)}_t\}_{6 \times 6} = 
\left( \begin{array}{cc}
\mathbf{I}_{3 \times 3} &amp; 0 \\
\Delta t~\mathbf{I}_{3 \times 3} &amp; \mathbf{I}_{3 \times 3}
\end{array} \right)\]</span></p>
<p><span class="math display">\[\mathbf{B}_t\{\boldsymbol{\theta}^{(i)}_t\}_{6 \times 3} = 
\left( \begin{array}{c}
\mathbf{R}_{b2f}\{\mathbf{q}^{(i)}_{t}\}\mathbf{a_A} \\
\mathbf{0}_{3 \times 3} \\
\end{array} \right)\]</span></p>
<p><span class="math display">\[\mathbf{Q}_t\{\boldsymbol{\theta}^{(i)}_t\}_{6 \times 6} = 
\left( \begin{array}{cc}
\mathbf{R}_{b2f}\{\mathbf{q}^{(i)}_{t}\}(\mathbf{Q}_{\mathbf{a}_t } * dt^2)\mathbf{R}^t_{b2f}\{\mathbf{q}^{(i)}_{t}\} &amp; \\
&amp; \mathbf{Q}_{\mathbf{v}_t }\\
\end{array} \right)\]</span></p>
<p><span class="math display">\[\hat{\mathbf{x}}^{-(i)}_t = \mathbf{F}_t\{\boldsymbol{\theta}^{(i)}_t\} \mathbf{x}^{(i)}_{t-1} + \mathbf{B}_t\{\boldsymbol{\theta}^{(i)}_t\} \mathbf{u}_t \]</span> <span class="math display">\[ \mathbf{\Sigma}^{-(i)}_t = \mathbf{F}_t\{\boldsymbol{\theta}^{(i)}_t\} \mathbf{\Sigma}^{-(i)}_{t-1}  (\mathbf{F}_t\{\boldsymbol{\theta}^{(i)}_t\})^T + \mathbf{Q}_t\{\boldsymbol{\theta}^{(i)}_t\}\]</span></p>
<h3 id="kalman-measurement-update">Kalman measurement update</h3>
<p>The <a href="#measurements-model-1">measurement model</a> defines how to compute <span class="math inline">\(p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1}, \mathbf{y}_{1:t-_K1})\)</span></p>
<p>Indeed, The measurement model defines the observation matrix <span class="math inline">\(\mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span>, the observation noise <span class="math inline">\(\mathbf{v}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span> and its covariance matrix <span class="math inline">\(\mathbf{R}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span> for the Kalman filter.</p>
<p><span class="math display">\[(\mathbf{a_A}_t, \mathbf{p_V}_t)^T  = \mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\} (\mathbf{v}_t, \mathbf{p}_t)^T + \mathbf{v}_t\{\boldsymbol{\theta}^{(i)}_t\}\]</span></p>
<p><span class="math display">\[\mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\}_{6 \times 3} = 
\left( \begin{array}{cc}
\mathbf{0}_{3 \times 3} &amp; \\
&amp; \mathbf{I}_{3 \times 3} \\
\end{array} \right)\]</span></p>
<p><span class="math display">\[\mathbf{R}_t\{\boldsymbol{\theta}^{(i)}_t\}_{3 \times 3} = 
\left( \begin{array}{c}
\mathbf{R}_{\mathbf{p_V}_t} 
\end{array} \right)\]</span></p>
<h3 id="kalman-update-1">Kalman update</h3>
<p><span class="math display">\[\mathbf{S} = \mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\} \mathbf{\Sigma}^{-(i)}_t  (\mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\})^T + \mathbf{R}_t\{\boldsymbol{\theta}^{(i)}_t\}\]</span> <span class="math display">\[\hat{\mathbf{z}} = \mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\}  \hat{\mathbf{x}}^{-(i)}_t\]</span> <span class="math display">\[\mathbf{K} = \mathbf{\Sigma}^{-(i)}_t \mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\}^T \mathbf{S}^{-1}\]</span> <span class="math display">\[\mathbf{\Sigma}^{(i)}_t = \mathbf{\Sigma}^{-(i)}_t + \mathbf{K} \mathbf{S} \mathbf{K}^T\]</span> <span class="math display">\[\hat{\mathbf{x}}^{(i)}_t = \hat{\mathbf{x}}^{-(i)}_t  + \mathbf{K}((\mathbf{a_A}_t, \mathbf{p_V}_t)^T - \hat{\mathbf{z}})\]</span> <span class="math display">\[p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1}, \mathbf{y}_{1:t-1}) = \mathcal{N}((\mathbf{a_A}_t, \mathbf{p_V}_t)^T; \hat{\mathbf{z}}_t, \mathbf{S})\]</span></p>
<h3 id="asynchronous-measurements">Asynchronous measurements</h3>
<p>Our measurements might have different sampling rate so instead of doing full kalman update, we only apply a partial kalman update corresponding to the current type of measurement <span class="math inline">\(\mathbf{z}_t\)</span>.</p>
<p>For indoor, there is only one kind of sensor for the Kalman update: <span class="math inline">\(\mathbf{p_V}\)</span></p>
<h3 id="attitude-reweighting">Attitude reweighting</h3>
<p>In the <a href="#measurements-model">measurement model</a>, the attitude defines another reweighting for importance sampling.</p>
<p><span class="math display">\[p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1}, \mathbf{y}_{1:t-1}) = \mathcal{N}(Q2R({\mathbf{q}^{(i)}}^{-1}\mathbf{q_V}_t);~ 0 ,~ \mathbf{R}_{\mathbf{q_V}})\]</span></p>
<h2 id="algorithm-summary">Algorithm summary</h2>
<ol style="list-style-type: decimal">
<li>Initiate <span class="math inline">\(N\)</span> particles with <span class="math inline">\(\mathbf{x}_0\)</span>, <span class="math inline">\(\mathbf{q}_0 ~ \sim p(\mathbf{q}_0)\)</span>, <span class="math inline">\(\mathbf{\Sigma}_0\)</span> and <span class="math inline">\(w = 1/N\)</span></li>
<li>While new sensor measurements <span class="math inline">\((\mathbf{z}_t, \mathbf{u}_t)\)</span></li>
</ol>
<ul>
<li>foreach <span class="math inline">\(N\)</span> particles <span class="math inline">\((i)\)</span>:
<ol style="list-style-type: decimal">
<li>Depending on the type of observation:
<ul>
<li><strong>IMU</strong>:
<ol style="list-style-type: decimal">
<li>store <span class="math inline">\(\boldsymbol{\mathbf{\omega_G}}_t\)</span> and <span class="math inline">\(\mathbf{a_A}_t\)</span> as last control inputs</li>
<li>sample new latent variable <span class="math inline">\(\boldsymbol{\theta_t}\)</span> from <span class="math inline">\(\boldsymbol{\mathbf{\omega_G}}_t\)</span> (which correspond to the last control inputs)</li>
<li>apply kalman prediction from <span class="math inline">\(\mathbf{a_A}_t\)</span> (which correspond to the last control inputs)</li>
</ol></li>
<li><strong>Vicon</strong>:
<ol style="list-style-type: decimal">
<li>sample new latent variable <span class="math inline">\(\boldsymbol{\theta_t}\)</span> from <span class="math inline">\(\boldsymbol{\mathbf{\omega_G}}_t\)</span> (which correspond to the last control inputs)</li>
<li>apply kalman prediction from <span class="math inline">\(\mathbf{a_A}_t\)</span> (which correspond to the last control inputs)</li>
<li>Partial kalman update with: <span class="math display">\[\mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\}_{3 \times 6} = (\mathbf{0}_{3 \times 3} ~~~~ \mathbf{I}_{3 \times 3} )\]</span> <span class="math display">\[\mathbf{R}_t\{\boldsymbol{\theta}^{(i)}_t\}_{3 \times 3} =  \mathbf{R}_{\mathbf{p_V}_t }\]</span> <span class="math display">\[\mathbf{x}^{(i)}_t = \mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\} \mathbf{x}^{(i)}_{t-1} + \mathbf{K}(\mathbf{p_V}_t - \hat{\mathbf{z}})\]</span> <span class="math display">\[p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1}, \mathbf{y}_{1:t-1}) = \mathcal{N}(\mathbf{q_V}_t; \mathbf{q}^{(i)}_t,~ \mathbf{R}_{\mathbf{q_V}_t } )\mathcal{N}(\mathbf{p_V}_t; \hat{\mathbf{z}}_t, \mathbf{S})\]</span></li>
</ol></li>
<li><strong>Other sensors (Outdoor)</strong>: As for <strong>Vicon</strong> but use the corresponding partial kalman update</li>
</ul></li>
<li>Update <span class="math inline">\(w^{(i)}_t\)</span>: <span class="math inline">\(w^{(i)}_t = p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1}, \mathbf{y}_{1:t-1}) w^{(i)}_{t-1}\)</span><br />
</li>
</ol></li>
<li>Normalize all <span class="math inline">\(w^{(i)}\)</span> by scalaing by <span class="math inline">\(1/(\sum w^{(i)})\)</span> such that <span class="math inline">\(\sum w^{(i)}= 1\)</span></li>
<li>Compute <span class="math inline">\(\mathbf{p}_t\)</span> and <span class="math inline">\(\mathbf{q}_t\)</span> as the expectation from the distribution approximated by the N particles.</li>
<li>Resample if the number of effective particle is too low</li>
</ul>
<h3 id="extension-to-outdoors">Extension to outdoors</h3>
<p>As highlighted in the Algorithm summary, the RPBF if easily extensible to other sensors. Indeed, measurements are either:</p>
<ul>
<li>giving information about position or velocity and their update is similar to the vicon position update as a kalman partial update</li>
<li>giving information about the orientation and their update is similar to the vicon attitude update as a pure importance sampling reweighting.</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p><strong>TODO</strong></p>
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-doucet_tutorial_2009">
<p>Doucet, Arnaud, and Adam M. Johansen. 2009. “A Tutorial on Particle Filtering and Smoothing: Fifteen Years Later.” <em>Handbook of Nonlinear Filtering</em> 12 (656-704): 3. <a href="http://www.warwick.ac.uk/fac/sci/statistics/staff/academic-research/johansen/publications/dj11.pdf" class="uri">http://www.warwick.ac.uk/fac/sci/statistics/staff/academic-research/johansen/publications/dj11.pdf</a>.</p>
</div>
<div id="ref-edgar_quaternion-based_nodate">
<p>Edgar, Kraft. n.d. “A Quaternion-Based Unscented Kalman Filter for Orientation Tracking.”</p>
</div>
<div id="ref-markley_averaging_2007">
<p>Markley, F. Landis, Yang Cheng, John Lucas Crassidis, and Yaakov Oshman. 2007. “Averaging Quaternions.” <em>Journal of Guidance, Control, and Dynamics</em> 30 (4): 1193–7. <a href="http://arc.aiaa.org/doi/abs/10.2514/1.28949" class="uri">http://arc.aiaa.org/doi/abs/10.2514/1.28949</a>.</p>
</div>
<div id="ref-vernaza_rao-blackwellized_2006">
<p>Vernaza, Paul, and Daniel D. Lee. 2006. “Rao-Blackwellized Particle Filtering for 6-DOF Estimation of Attitude and Position via GPS and Inertial Sensors.” In <em>Robotics and Automation, 2006. ICRA 2006. Proceedings 2006 IEEE International Conference on</em>, 1571–8. IEEE. <a href="http://ieeexplore.ieee.org/abstract/document/1641931/" class="uri">http://ieeexplore.ieee.org/abstract/document/1641931/</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The observation that the number of transistors in a dense integrated circuit doubles approximately every two years.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>The etymology for “Dead reckoning” comes from the mariners of the XVIIth century that used to calculate the position of the vessel with log book. The interpretation of “dead” is subject to debate. Some argue that it is a mispelling of “ded” as in “deduced”. Others argue that it should be read by its old meaning: <em>absolute</em>.<a href="#fnref2">↩</a></p></li>
</ol>
</div>
</body>
</html>
